{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7WupX1X-rB",
        "outputId": "174c6d70-13f5-4dd9-e663-20f4d3eb3cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "## Initializations\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Data\n",
        "import pandas as pd\n",
        "csv_file_path = 'init_input.csv'\n",
        "sample_count = 2\n",
        "data = pd.read_csv(csv_file_path, header=0)\n",
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAJ8jGu8YSom",
        "outputId": "6e8bd038-e53c-4e58-a901-77dcf20069b6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Type', 'Source'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
      ],
      "metadata": {
        "id": "vKxTwqxdZCOs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "## Translate data\n",
        "language_source = 'en'\n",
        "# language_target_names = ['German', 'Greek', 'English', 'Spanish', 'Persian',\n",
        "#                          'French', 'Hindi', 'Croatian', 'Italian', 'Korean',\n",
        "#                          'Dutch', 'Romanian', 'Russian', 'Turkish', 'Chinese']\n",
        "# language_target_labels = ['de', 'el', 'en', 'es', 'fa', 'fr', 'hi', 'hr', 'it',\n",
        "#                           'ko', 'nl', 'ro', 'ru', 'tr', 'zh']\n",
        "language_target_names = ['Persian', 'Dutch']\n",
        "language_target_labels = ['fa', 'nl']\n",
        "\n",
        "result_fw = []\n",
        "hiddens_fw_avg = []\n",
        "hiddens_fw_last = []\n",
        "for i, r in data.iterrows():\n",
        "  label = r.iloc[1]\n",
        "  sentence = r.iloc[2]\n",
        "  for l_name, l_label in zip(language_target_names, language_target_labels):\n",
        "    tokenizer.src_lang = 'en'\n",
        "    if l_label != 'en':\n",
        "      input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "      generated_tokens = model.generate(input_ids,\n",
        "                                        forced_bos_token_id=tokenizer.get_lang_id(l_label),\n",
        "                                        output_hidden_states=True)\n",
        "      output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "      hidden_states = (model.get_encoder()(input_ids).last_hidden_state)\n",
        "      hidden_avg = torch.mean(hidden_states, dim=1)[0,:]\n",
        "      hidden_last = hidden_states[0,-1,:]\n",
        "      hiddens_fw_avg.append(hidden_avg)\n",
        "      hiddens_fw_last.append(hidden_last)\n",
        "    else:\n",
        "      output = sentence\n",
        "    result_fw.append([i, label, l_label, l_name, sentence, ''.join(output) ])\n",
        "df_result_fw = pd.DataFrame(result_fw)\n",
        "print(df_result_fw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVzZnJ22ap2C",
        "outputId": "653ad6ca-913d-466d-df51-183fbffabe1a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0  1   2        3                                                  4  \\\n",
            "0   0  2  fa  Persian  Andrei and Danny picked up the yellow chair an...   \n",
            "1   0  2  nl    Dutch  Andrei and Danny picked up the yellow chair an...   \n",
            "2   1  2  fa  Persian   Andrei looked at Danny putting down a yellow bag   \n",
            "3   1  2  nl    Dutch   Andrei looked at Danny putting down a yellow bag   \n",
            "4   2  3  fa  Persian             Andrei approached Danny; he held a bag   \n",
            "5   2  3  nl    Dutch             Andrei approached Danny; he held a bag   \n",
            "6   3  1  fa  Persian              Andrei and Danny moved a yellow chair   \n",
            "7   3  1  nl    Dutch              Andrei and Danny moved a yellow chair   \n",
            "8   4  1  fa  Persian                     Yevgeni left Andrei; Danny too   \n",
            "9   4  1  nl    Dutch                     Yevgeni left Andrei; Danny too   \n",
            "10  5  3  fa  Persian  Danny put down the bag and the chair; it was g...   \n",
            "11  5  3  nl    Dutch  Danny put down the bag and the chair; it was g...   \n",
            "\n",
            "                                                    5  \n",
            "0           آندری و دنی صندلی زرد و کیسه را برداشتند.  \n",
            "1   Andrei en Danny nemen de gele stoel en de koff...  \n",
            "2    آندری به دنی نگاه کرد و چمدان زرد را پایین آورد.  \n",
            "3    Andrei keek Danny naar beneden met een gele tas.  \n",
            "4        آندری به دانی نزدیک شد؛ او چمدان را نگه داشت  \n",
            "5           Andrei keek naar Danny; hij hield een tas  \n",
            "6                     آندری و دانی صندلی زرد می پوشند  \n",
            "7          Andrei en Danny verplaatsen een gele stoel  \n",
            "8                   یوجینی آندری را ترک کرد؛ دانی نیز  \n",
            "9                   Yevgeni verlaat Andrei; Danny ook  \n",
            "10        دانی کیسه و صندلی را پایین انداخت؛ سبز بود.  \n",
            "11  Danny legde de tas en de stoel neer; het was g...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "## Translate backward data\n",
        "\n",
        "result_bw = []\n",
        "hiddens_bw_avg = []\n",
        "hiddens_bw_last = []\n",
        "for i, r in df_result_fw.iterrows():\n",
        "  label = r.iloc[1]\n",
        "  sentence = r.iloc[5]\n",
        "  l_name = r.iloc[3]\n",
        "  l_label = r.iloc[2]\n",
        "\n",
        "  tokenizer.src_lang = l_label\n",
        "  if l_label != 'en':\n",
        "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "    generated_tokens = model.generate(input_ids,\n",
        "                                      forced_bos_token_id=tokenizer.get_lang_id('en'),\n",
        "                                      output_hidden_states=True)\n",
        "    output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    hidden_states = (model.get_encoder()(input_ids).last_hidden_state)\n",
        "    hidden_avg = torch.mean(hidden_states, dim=1)[0,:]\n",
        "    hidden_last = hidden_states[0,-1,:]\n",
        "    hiddens_bw_avg.append(hidden_avg)\n",
        "    hiddens_bw_last.append(hidden_last)\n",
        "  else:\n",
        "    output = sentence\n",
        "  result_bw.append([i, label, l_label, l_name, sentence, ''.join(output) ])\n",
        "df_result_bw = pd.DataFrame(result_bw)\n",
        "print(df_result_bw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umv4GX16sSFR",
        "outputId": "1e0029f6-3151-444a-87c2-00852e4385db"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0  1   2        3                                                  4  \\\n",
            "0    0  2  fa  Persian          آندری و دنی صندلی زرد و کیسه را برداشتند.   \n",
            "1    1  2  nl    Dutch  Andrei en Danny nemen de gele stoel en de koff...   \n",
            "2    2  2  fa  Persian   آندری به دنی نگاه کرد و چمدان زرد را پایین آورد.   \n",
            "3    3  2  nl    Dutch   Andrei keek Danny naar beneden met een gele tas.   \n",
            "4    4  3  fa  Persian       آندری به دانی نزدیک شد؛ او چمدان را نگه داشت   \n",
            "5    5  3  nl    Dutch          Andrei keek naar Danny; hij hield een tas   \n",
            "6    6  1  fa  Persian                    آندری و دانی صندلی زرد می پوشند   \n",
            "7    7  1  nl    Dutch         Andrei en Danny verplaatsen een gele stoel   \n",
            "8    8  1  fa  Persian                  یوجینی آندری را ترک کرد؛ دانی نیز   \n",
            "9    9  1  nl    Dutch                  Yevgeni verlaat Andrei; Danny ook   \n",
            "10  10  3  fa  Persian        دانی کیسه و صندلی را پایین انداخت؛ سبز بود.   \n",
            "11  11  3  nl    Dutch  Danny legde de tas en de stoel neer; het was g...   \n",
            "\n",
            "                                                    5  \n",
            "0      Andri and Danny took a yellow chair and a bag.  \n",
            "1   Andrei and Danny take the yellow chair and the...  \n",
            "2   Andrew looked at Danny and brought down the ye...  \n",
            "3      Andrei looked down to Danny with a yellow bag.  \n",
            "4   And Andrew came to know, and he kept his baggage.  \n",
            "5               Andrei looked at Danny; he kept a bag  \n",
            "6                Andri and Danny wear a yellow chair.  \n",
            "7                Andrei and Danny move a yellow chair  \n",
            "8                    And he left Andrew, and he knew.  \n",
            "9                   Yevgeni leaves Andrei; Danny also  \n",
            "10    He dropped the bag and the chair, it was green.  \n",
            "11  Danny put the bag and the chair down; it was g...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save output\n",
        "import pandas as pd\n",
        "\n",
        "df_result_fw.to_csv(\"language_translate_fw_output_sample\"+str(sample_count)+\".csv\",\n",
        "                 index=False, encoding='utf-8')\n",
        "df_result_bw.to_csv(\"language_translate_bw_output_sample\"+str(sample_count)+\".csv\",\n",
        "                 index=False, encoding='utf-8')\n",
        "\n",
        "try:\n",
        "  hiddens_fw_avg = torch.stack(hiddens_fw_avg)\n",
        "  hiddens_fw_last = torch.stack(hiddens_fw_last)\n",
        "  hiddens_bw_avg = torch.stack(hiddens_bw_avg)\n",
        "  hiddens_bw_last = torch.stack(hiddens_bw_last)\n",
        "except Exception as e:\n",
        "    print(\"Tensors already stacked!\")\n",
        "\n",
        "torch.save(hiddens_fw_avg, \"language_translate_fw_avg_output_sample\"+str(sample_count)+\".pth\")\n",
        "torch.save(hiddens_fw_last, \"language_translate_fw_last_output_sample\"+str(sample_count)+\".pth\")\n",
        "torch.save(hiddens_bw_avg, \"language_translate_bw_avg_output_sample\"+str(sample_count)+\".pth\")\n",
        "torch.save(hiddens_bw_last, \"language_translate_bw_last_output_sample\"+str(sample_count)+\".pth\")\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# List of files to be included in the zip file\n",
        "files_to_zip = [\"language_translate_fw_output_sample\"+str(sample_count)+\".csv\",\n",
        "                \"language_translate_bw_output_sample\"+str(sample_count)+\".csv\",\n",
        "                \"language_translate_fw_avg_output_sample\"+str(sample_count)+\".pth\",\n",
        "                \"language_translate_fw_last_output_sample\"+str(sample_count)+\".pth\",\n",
        "                \"language_translate_bw_avg_output_sample\"+str(sample_count)+\".pth\",\n",
        "                \"language_translate_bw_last_output_sample\"+str(sample_count)+\".pth\"]\n",
        "\n",
        "# Path where the zip file will be created\n",
        "zip_file_path = \"language_translate_output_sample\"+str(sample_count)+\".zip\"\n",
        "\n",
        "# Create a new zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    # Add each file to the zip file\n",
        "    for file in files_to_zip:\n",
        "        zipf.write(file)\n",
        "\n",
        "print(\"Zip file created successfully:\", zip_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVtxQ_0QuThI",
        "outputId": "fcf41f7b-eda8-465c-f66e-87e91a5b6265"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file created successfully: language_translate_output_sample2.zip\n"
          ]
        }
      ]
    }
  ]
}