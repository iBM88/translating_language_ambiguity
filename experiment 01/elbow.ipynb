{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N9peVzlOFqK",
    "outputId": "d6030b3c-996f-4046-9f59-fa9c157d480f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 1 1 1 3 3 1 3 1 1 1 1 1 3 1 1 1 3 1 1 3 1 3 1 1 1 1 1 1 1 1 3 1 1 3 1 1 1\n",
      " 1 1 1 3 1 1 3 3 1 1 1 3 1 3 3 3 1 3 3 1 1 1 3 3 1 3 3 1 1 1 3 3 1 3 1 1 3\n",
      " 3 3 3 1 3 3 3 1 1 3 1 3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Index(['0', '1', '2', '3', '4', '5'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load Hidden states\n",
    "import torch\n",
    "tensor_fw_avg = torch.load('language_translate_fw_avg_output_sample0.pth')\n",
    "tensor_bw_avg = torch.load('language_translate_bw_avg_output_sample0.pth')\n",
    "\n",
    "tensor_fw_avg = torch.tensor(tensor_fw_avg)\n",
    "tensor_bw_avg = torch.tensor(tensor_bw_avg)\n",
    "\n",
    "## Load Data\n",
    "import pandas as pd\n",
    "csv_file_path = 'language_translate_fw_output_sample0.csv'\n",
    "data = pd.read_csv(csv_file_path, header=0)\n",
    "languages = data['3'].unique()\n",
    "types = data[data['3'] == languages[0]]['1'].values\n",
    "print(types)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ki7ehst5PQPH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, latent_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(model, inputs, target, num_epochs = 20, lr_=0.001):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_)\n",
    "\n",
    "    losses = []  # List to store losses for plotting\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)  # Compute the reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # Store loss for plotting\n",
    "        losses.append(loss.item())\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(tensor_fw_avg)}\")\n",
    "\n",
    "    # print(\"Finished Training\")\n",
    "    return losses\n",
    "\n",
    "def get_error(model, inputs, targets):\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    running_loss = 0.0\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)  # Compute the reconstruction loss\n",
    "    # print('loss:', loss.shape)\n",
    "    loss_2_sum = torch.sum(loss, 1) / loss.shape[1]\n",
    "    # print('loss_2_sum:', loss_2_sum.shape)\n",
    "    outputs_sum = torch.sum(outputs, 1) #/ outputs.shape[1]\n",
    "    # print('outputs_sum:', outputs_sum.shape)\n",
    "    targets_sum = torch.sum(targets, 1) #/ targets.shape[1]\n",
    "    # print('targets_sum:', targets_sum.shape)\n",
    "    errors = loss_2_sum / (outputs_sum * targets_sum)\n",
    "    # print('errors:', errors.shape)\n",
    "    return torch.abs(errors)\n",
    "    # return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAlDaUT4ZPQ6",
    "outputId": "24b5b341-6546-43df-c7ae-4e8a7584b59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/bmehrparvar.6222928/ipykernel_2551418/4266787441.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fw = torch.tensor(fw, requires_grad=True)\n",
      "/scratch-local/bmehrparvar.6222928/ipykernel_2551418/4266787441.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bw = torch.tensor(bw, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German\n",
      "Greek\n",
      "Spanish\n",
      "Persian\n",
      "French\n",
      "Hindi\n",
      "Croatian\n",
      "Italian\n",
      "Korean\n",
      "Dutch\n",
      "Romanian\n",
      "Russian\n",
      "Turkish\n",
      "Chinese\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_size = tensor_fw_avg.shape[1]  # Size of vectors\n",
    "losses_all = {}\n",
    "num_epochs = 300\n",
    "latents = np.arange(8, 64, 8)\n",
    "# latents = [64, 32]\n",
    "matrices = np.empty((0, 5))\n",
    "\n",
    "for lang in languages:\n",
    "    for latent_size in latents:\n",
    "        index = list(data[data['3'] == lang].index)\n",
    "        fw = tensor_fw_avg[index]\n",
    "        fw = torch.tensor(fw, requires_grad=True)\n",
    "        bw = tensor_bw_avg[index]\n",
    "        bw = torch.tensor(bw, requires_grad=True)\n",
    "\n",
    "        model = Autoencoder(input_size, latent_size)\n",
    "        losses = train_autoencoder(model, fw, bw, num_epochs=num_epochs)\n",
    "        losses_all[lang + ' (hid=' + str(latent_size) + ')'] = losses\n",
    "\n",
    "        errors = get_error(model, fw, bw).detach().numpy()\n",
    "        ids = list(range(errors.shape[0]))\n",
    "        langs = np.repeat(lang, errors.shape[0])\n",
    "        latent_sizes = np.repeat(latent_size, errors.shape[0]) / fw.shape[1]\n",
    "        matrix = np.vstack((ids, types, langs, errors, latent_sizes))\n",
    "        matrices = np.concatenate((matrices, matrix.T), axis=0)\n",
    "        \n",
    "    print(lang)\n",
    "# print(losses_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QI7cCQ15sd9n",
    "outputId": "ff75cad8-79b6-4329-adc6-8b268dc34bd5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_error(losses, title):\n",
    "    plt.clf()\n",
    "    for key, value in losses.items():\n",
    "        plt.plot(value, label=key)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=2)\n",
    "    plt.savefig('loss.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_elbow(dataframe, title='Elbow chart', id_=0):\n",
    "    plt.clf()\n",
    "    # Get unique categories\n",
    "    unique_categories = np.unique(dataframe[2].unique())\n",
    "    num_categories = len(unique_categories)\n",
    "\n",
    "    # Define a colormap with enough distinct colors\n",
    "    cmap = plt.get_cmap('tab10', num_categories)\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=num_categories - 1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    scatter_plots = {}\n",
    "    for id in dataframe[0].unique():\n",
    "        if id == id_:\n",
    "            for i, lang in enumerate(dataframe[2].unique()):\n",
    "                if True: #lang == 'Persian':\n",
    "                    df = dataframe[(dataframe[0] == id) & (dataframe[2] == lang)]\n",
    "\n",
    "                    data = {'X': df[4].values,\n",
    "                            'Y': df[3].values}\n",
    "                    df = pd.DataFrame(data)\n",
    "\n",
    "                    filter = df.sort_values(by='X')\n",
    "                    # plt.scatter(filter[4].values, filter[3].values, color=cmap(i))\n",
    "                    scatter_plots[lang], = plt.plot(filter['X'], filter['Y'],\n",
    "                                                    linestyle='-', marker='o',\n",
    "                                                    color=cmap(i), label=lang)\n",
    "\n",
    "    plt.xlabel('Complexity (c)')\n",
    "    plt.ylabel('error (e)')\n",
    "    plt.title('%s item: %s' % (title, id_))\n",
    "    plt.legend(handles=[scatter_plots[cat] for cat in unique_categories])\n",
    "    plt.grid(True)\n",
    "    plt.savefig('elbow_s%s.png' % id_)\n",
    "    plt.show()\n",
    "\n",
    "# plot_error(losses_all, 'Training Loss over Iterations')\n",
    "df_matrices = pd.DataFrame(matrices)\n",
    "df_matrices[0] = df_matrices[0].astype(int)\n",
    "df_matrices[1] = df_matrices[1].astype(int)\n",
    "df_matrices[2] = df_matrices[2].astype(str)\n",
    "df_matrices[3] = df_matrices[3].astype(float)\n",
    "df_matrices[4] = df_matrices[4].astype(float)\n",
    "\n",
    "# plot_elbow(df_matrices, 'Elbow chart (training epochs=%s)' % num_epochs)\n",
    "# print(df_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8FCLC7vkwsdN"
   },
   "outputs": [],
   "source": [
    "df_matrices.to_csv('elbow_avg_output_sample0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0546875\n",
      "(474, 198)\n",
      "(474, 16)\n"
     ]
    }
   ],
   "source": [
    "matrices_raw = np.empty((0, 2 + 2 * latents.shape[0] * len(languages)))\n",
    "matrices_raw_best = np.empty((0, 2 + len(languages)))\n",
    "mx = max(df_matrices[4].values)\n",
    "print(mx)\n",
    "for id in df_matrices[0].unique():\n",
    "    filtered = df_matrices[df_matrices[0]==id]\n",
    "    filtered_best = df_matrices[(df_matrices[0]==id) & (df_matrices[4]==mx)]\n",
    "    row = np.hstack((id,filtered.iloc[0][1], filtered[3].T.values, filtered[4].T.values))\n",
    "    matrices_raw = np.vstack((matrices_raw, row))\n",
    "        \n",
    "    row_best = np.hstack((id,filtered_best.iloc[0][1], filtered_best[3].T.values))\n",
    "    matrices_raw_best = np.vstack((matrices_raw_best, row_best))\n",
    "\n",
    "df_matrices_raw = pd.DataFrame(matrices_raw)\n",
    "df_matrices_raw_best = pd.DataFrame(matrices_raw_best)\n",
    "df_matrices_raw[0] = df_matrices_raw[0].astype(int)\n",
    "df_matrices_raw[1] = df_matrices_raw[1].astype(int)\n",
    "df_matrices_raw_best[0] = df_matrices_raw_best[0].astype(int)\n",
    "df_matrices_raw_best[1] = df_matrices_raw_best[1].astype(int)\n",
    "for i in range(len(df_matrices_raw.columns)):\n",
    "    if i > 2:\n",
    "        df_matrices_raw[i] = df_matrices_raw[i].astype(float)\n",
    "for i in range(len(df_matrices_raw_best.columns)):\n",
    "    if i > 2:\n",
    "        df_matrices_raw_best[i] = df_matrices_raw_best[i].astype(float)\n",
    "print(df_matrices_raw.shape)\n",
    "print(df_matrices_raw_best.shape)\n",
    "df_matrices_raw.to_csv('elbow_avg_output_sample0_raw.csv', index=False)\n",
    "df_matrices_raw_best.to_csv('elbow_avg_output_sample0_raw_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(losses_all, 'Training Loss over Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow(df_matrices, 'Elbow chart (training epochs=%s)' % num_epochs, id_=0)\n",
    "plot_elbow(df_matrices, 'Elbow chart (training epochs=%s)' % num_epochs, id_=237)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
